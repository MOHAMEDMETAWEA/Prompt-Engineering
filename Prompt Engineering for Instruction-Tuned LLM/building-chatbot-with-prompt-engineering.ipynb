{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "921a2544",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.009972,
     "end_time": "2024-03-03T18:06:37.521236",
     "exception": false,
     "start_time": "2024-03-03T18:06:37.511264",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <center style=\"font-family: consolas; font-size: 32px; font-weight: bold;\">  Prompt Engineering Best Practices: BuildingÂ Chatbots </center>\n",
    "\n",
    "# <center style=\"font-family: consolas; font-size: 25px; font-weight: bold;\">  Prompt Engineering for Instruction-Tuned LLMs </center>\n",
    "***\n",
    "\n",
    "One of the compelling aspects of utilizing a large language model lies in its capacity to effortlessly construct a personalized chatbot and leverage it to craft your very own chatbot tailored to various applications.Â \n",
    "\n",
    "In the forthcoming tutorial, we delve deep into the OpenAI chat completions format, unraveling its nuances and intricacies to provide you with a comprehensive understanding.Â \n",
    "\n",
    "Armed with this knowledge, you'll embark on an enlightening journey towards constructing your very own chatbot from the ground up. Through step-by-step guidance and practical demonstrations, you'll unlock the potential to shape conversational experiences that resonate with your audience, driving engagement and efficiency in your chosen domain.\n",
    "\n",
    "\n",
    " #### <a id=\"top\"></a>\n",
    "\n",
    "# <div style=\"box-shadow: rgb(60, 121, 245) 0px 0px 0px 3px inset, rgb(255, 255, 255) 10px -10px 0px -3px, rgb(31, 193, 27) 10px -10px, rgb(255, 255, 255) 20px -20px 0px -3px, rgb(255, 217, 19) 20px -20px, rgb(255, 255, 255) 30px -30px 0px -3px, rgb(255, 156, 85) 30px -30px, rgb(255, 255, 255) 40px -40px 0px -3px, rgb(255, 85, 85) 40px -40px; padding:20px; margin-right: 40px; font-size:30px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(60, 121, 245);\"><b>Table of contents</b></div>\n",
    "\n",
    "<div style=\"background-color: rgba(60, 121, 245, 0.03); padding:30px; font-size:15px; font-family: consolas;\">\n",
    "<ul>\n",
    "    <li><a href=\"#1\" target=\"_self\" rel=\" noreferrer nofollow\">1. Setting Working Environment & Getting Started </a> </li>\n",
    "    <li><a href=\"#2\" target=\"_self\" rel=\" noreferrer nofollow\">2. Understanding Messages Roles </a></li>\n",
    "    <li><a href=\"#3\" target=\"_self\" rel=\" noreferrer nofollow\">3. Build a Customized Chatbot </a></li>     \n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "***\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae404df",
   "metadata": {
    "papermill": {
     "duration": 0.008782,
     "end_time": "2024-03-03T18:06:37.539096",
     "exception": false,
     "start_time": "2024-03-03T18:06:37.530314",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"1\"></a>\n",
    "# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 1. Setting Working Environment & GettingÂ Started </b></div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We will use the OpenAI Python library to access the OpenAI API. You can use this Python library using pip like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9da00ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T12:15:33.250090Z",
     "iopub.status.busy": "2025-12-31T12:15:33.249591Z",
     "iopub.status.idle": "2025-12-31T12:15:34.926253Z",
     "shell.execute_reply": "2025-12-31T12:15:34.925616Z"
    },
    "papermill": {
     "duration": 16.539134,
     "end_time": "2024-03-03T18:06:54.087056",
     "exception": false,
     "start_time": "2024-03-03T18:06:37.547922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\administrator\\anaconda3\\lib\\site-packages (2.13.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from openai) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from openai) (0.12.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from openai) (2.12.5)\n",
      "Requirement already satisfied: sniffio in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61608816",
   "metadata": {
    "papermill": {
     "duration": 0.009773,
     "end_time": "2024-03-03T18:06:54.106904",
     "exception": false,
     "start_time": "2024-03-03T18:06:54.097131",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Next, we will import OpenAI and then set the OpenAI API key as a secret key. You can get one of these API keys from the OpenAI website. Setting this as an environment variable is better to keep it safe if you share your code. We will use OpenAI's chatGPT GPT 3.5 Turbo model, and the chat completions endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ee6809",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T12:15:34.929315Z",
     "iopub.status.busy": "2025-12-31T12:15:34.929109Z",
     "iopub.status.idle": "2025-12-31T12:15:41.233746Z",
     "shell.execute_reply": "2025-12-31T12:15:41.233046Z"
    },
    "papermill": {
     "duration": 1.375678,
     "end_time": "2024-03-03T18:06:55.493091",
     "exception": false,
     "start_time": "2024-03-03T18:06:54.117413",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpanel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpn\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n",
      "File \u001b[1;32md:\\Prompt Engineerings\\Prompt Engineering for Instruction-Tuned LLM\\utils.py:15\u001b[0m\n\u001b[0;32m     12\u001b[0m openai_api_base \u001b[38;5;241m=\u001b[39m env_values\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_BASE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m openai_api_name \u001b[38;5;241m=\u001b[39m env_values\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_NAME\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai/gpt-4o-mini\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI(\n\u001b[0;32m     16\u001b[0m     api_key\u001b[38;5;241m=\u001b[39mopenai_api_key,\n\u001b[0;32m     17\u001b[0m     base_url\u001b[38;5;241m=\u001b[39mopenai_api_base\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Mock moderation for OpenRouter compatibility\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m openai_api_base \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenrouter.ai\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m openai_api_base:\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\openai\\_client.py:137\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[1;34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[0;32m    135\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[0;32m    138\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    139\u001b[0m     )\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(api_key):\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import utils\n",
    "import panel as pn\n",
    "from openai import OpenAI\n",
    "from dotenv import dotenv_values\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ==============================\n",
    "# ðŸ”‘ Load environment variables\n",
    "# ==============================\n",
    "env_values = dotenv_values(\".env\")\n",
    "\n",
    "openai_api_key = env_values.get(\"OPENAI_API_KEY\")\n",
    "openai_api_base = env_values.get(\"OPENAI_API_BASE\")\n",
    "openai_api_name = env_values.get(\"OPENAI_API_NAME\")\n",
    "\n",
    "# Initialize Clients\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base\n",
    ")\n",
    "\n",
    "# Mock moderation for OpenRouter compatibility\n",
    "if openai_api_base and \"openrouter.ai\" in openai_api_base:\n",
    "    class MockResult(dict):\n",
    "        def __getattr__(self, name): return self.get(name)\n",
    "    class MockModeration(dict):\n",
    "        def __init__(self):\n",
    "            res = MockResult(flagged=False, categories={}, category_scores={})\n",
    "            super().__init__(results=[res])\n",
    "        @property\n",
    "        def results(self): return self[\"results\"]\n",
    "    client.moderations.create = lambda *args, **kwargs: MockModeration()\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model_name=openai_api_name, \n",
    "    openai_api_key=openai_api_key, \n",
    "    openai_api_base=openai_api_base,\n",
    "    temperature=0.7\n",
    ")\n",
    "pn.extension()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b2846f",
   "metadata": {
    "papermill": {
     "duration": 0.009877,
     "end_time": "2024-03-03T18:06:55.513200",
     "exception": false,
     "start_time": "2024-03-03T18:06:55.503323",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's define two helper functions the first one is one that we've been using throughout all the videos and it's the get_completion function if you kind of look at it we give a prompt but then inside the function what we're doing is putting this prompt into what looks like some kind of user message and this is because the chatGPT model is a chat model which means it's trained to take a series of messages as input and then return model generated messages output. Therefore the user message is the input and then the assistant messages are the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62023911",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T12:15:41.236664Z",
     "iopub.status.busy": "2025-12-31T12:15:41.236238Z",
     "iopub.status.idle": "2025-12-31T12:15:41.239603Z",
     "shell.execute_reply": "2025-12-31T12:15:41.239065Z"
    },
    "papermill": {
     "duration": 0.020461,
     "end_time": "2024-03-03T18:06:55.543858",
     "exception": false,
     "start_time": "2024-03-03T18:06:55.523397",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# ðŸ”§ Helper Functions\n",
    "# ==============================\n",
    "def get_completion(prompt, model=openai_api_name, temperature=0, max_tokens=500):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0426a9b8",
   "metadata": {
    "papermill": {
     "duration": 0.011021,
     "end_time": "2024-03-03T18:06:55.565128",
     "exception": false,
     "start_time": "2024-03-03T18:06:55.554107",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We also are going to use a new helper function instead of defining a single prompt as input and getting a single completion as a response. We are going to pass in a list of messages and these messages can be kind of from a variety of different roles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed45765d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T12:15:41.241573Z",
     "iopub.status.busy": "2025-12-31T12:15:41.241363Z",
     "iopub.status.idle": "2025-12-31T12:15:41.243821Z",
     "shell.execute_reply": "2025-12-31T12:15:41.243414Z"
    },
    "papermill": {
     "duration": 0.019612,
     "end_time": "2024-03-03T18:06:55.595387",
     "exception": false,
     "start_time": "2024-03-03T18:06:55.575775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_completion_from_messages(messages, model=openai_api_name, temperature=0, max_tokens=500):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1415d7",
   "metadata": {
    "papermill": {
     "duration": 0.010116,
     "end_time": "2024-03-03T18:06:55.616356",
     "exception": false,
     "start_time": "2024-03-03T18:06:55.606240",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"2\"></a>\n",
    "# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 2. Understanding MessagesÂ Roles </b></div>\n",
    "\n",
    "\n",
    "There are two main message types we will use. The first message is a system message which gives an overall instruction to the LLM. After this message, we have turns between the user and the assistant and this continues to go on. If you have used the web interface of chatGPT then your messages are the user messages and then ChatGPT's messages are the assistant messages.Â \n",
    "\n",
    "Therefore the system message helps to set the behavior and Persona of the assistant and it acts as a high-level instruction for the conversation. You can kind of think of it as whispering in the assistant's ear and guiding its responses without the user being aware of the system message. So as the user, if you've ever used ChatGPT you probably don't know what's in the ChatGPT system message.Â \n",
    "\n",
    "The benefit of the system message is that it allows you the developer to frame the conversation without making the request itself part of the conversation. So you can guide the assistant and whisper in its ear and guide its responses without making the user aware of it.Â \n",
    "Let's try to use these messages in a practical conversation to have a better understanding. We will use the  new helper function to get the completion from the messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1368d49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T12:15:41.246424Z",
     "iopub.status.busy": "2025-12-31T12:15:41.246203Z",
     "iopub.status.idle": "2025-12-31T12:15:41.250160Z",
     "shell.execute_reply": "2025-12-31T12:15:41.249574Z"
    },
    "papermill": {
     "duration": 0.019536,
     "end_time": "2024-03-03T18:06:55.646128",
     "exception": false,
     "start_time": "2024-03-03T18:06:55.626592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages =  [  \n",
    "{'role':'system', 'content':'You are an assistant that speaks like Shakespeare.'},    \n",
    "{'role':'user', 'content':'tell me a joke'},   \n",
    "{'role':'assistant', 'content':'Why did the chicken cross the road'},   \n",
    "{'role':'user', 'content':'I don\\'t know'}  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0747e240",
   "metadata": {
    "papermill": {
     "duration": 0.009579,
     "end_time": "2024-03-03T18:06:55.665709",
     "exception": false,
     "start_time": "2024-03-03T18:06:55.656130",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The system message says you are an assistant that speaks like Shakespeare. This is our description to the assistant on how it should behave. The first user message is \"Tell me a joke\" and the next system message is \"Why did the chicken cross the road\" and then the final user message is \"I don't know\". If we run this response we will get to the other side of the joke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "359a375c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T12:15:41.252415Z",
     "iopub.status.busy": "2025-12-31T12:15:41.252107Z",
     "iopub.status.idle": "2025-12-31T12:15:43.360638Z",
     "shell.execute_reply": "2025-12-31T12:15:43.359985Z"
    },
    "papermill": {
     "duration": 1.123555,
     "end_time": "2024-03-03T18:06:56.799173",
     "exception": false,
     "start_time": "2024-03-03T18:06:55.675618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To reach the other side, fair friend! Forsooth, â€˜tis but a jest of simple wit. Wouldst thou like to hear another, perchance?\n"
     ]
    }
   ],
   "source": [
    "response = get_completion_from_messages(messages, temperature=1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1915726e",
   "metadata": {
    "papermill": {
     "duration": 0.010293,
     "end_time": "2024-03-03T18:06:56.819446",
     "exception": false,
     "start_time": "2024-03-03T18:06:56.809153",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's take  another example in which the system message will be \" You are a friendly chatbot\" and the first user message is \"Hi, my name is Youssef\". So let's execute this to get the assistant message which will be \"Hello Youssef! It's nice to meet you. How are you doing today?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b41376d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T12:15:43.363105Z",
     "iopub.status.busy": "2025-12-31T12:15:43.362614Z",
     "iopub.status.idle": "2025-12-31T12:15:44.301529Z",
     "shell.execute_reply": "2025-12-31T12:15:44.300783Z"
    },
    "papermill": {
     "duration": 0.763289,
     "end_time": "2024-03-03T18:06:57.593319",
     "exception": false,
     "start_time": "2024-03-03T18:06:56.830030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Youssef! Itâ€™s great to meet you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "messages =  [  \n",
    "{'role':'system', 'content':'You are friendly chatbot.'},    \n",
    "{'role':'user', 'content':'Hi, my name is Youssef'}  ]\n",
    "response = get_completion_from_messages(messages, temperature=1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e6d898",
   "metadata": {
    "papermill": {
     "duration": 0.010425,
     "end_time": "2024-03-03T18:06:57.613736",
     "exception": false,
     "start_time": "2024-03-03T18:06:57.603311",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's try another example in which the system message is \"You are a friendly chatbot\" and the user message is \"Yes, can you remind me, What is my name?\"Â \n",
    "Let's get the response to this message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff5030e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T12:15:44.304286Z",
     "iopub.status.busy": "2025-12-31T12:15:44.304079Z",
     "iopub.status.idle": "2025-12-31T12:15:46.102751Z",
     "shell.execute_reply": "2025-12-31T12:15:46.102332Z"
    },
    "papermill": {
     "duration": 0.864706,
     "end_time": "2024-03-03T18:06:58.488568",
     "exception": false,
     "start_time": "2024-03-03T18:06:57.623862",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I donâ€™t have access to personal data about users unless you share it with me in the conversation. If you tell me your name, Iâ€™ll be happy to remember it for the rest of our chat!\n"
     ]
    }
   ],
   "source": [
    "messages =  [  \n",
    "{'role':'system', 'content':'You are friendly chatbot.'},    \n",
    "{'role':'user', 'content':'Yes,  can you remind me, What is my name?'}  ]\n",
    "response = get_completion_from_messages(messages, temperature=1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d14960",
   "metadata": {
    "papermill": {
     "duration": 0.009838,
     "end_time": "2024-03-03T18:06:58.509207",
     "exception": false,
     "start_time": "2024-03-03T18:06:58.499369",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "You can see the model doesn't know my name so each conversation with a language model is a standalone interaction which means that you must provide all relevant messages for the model to draw from in the current conversation.\n",
    "Therefore  if you want the model to remember earlier parts of the conversation you must provide the earlier exchanges in the input to the model and so we'll refer to this as context. Let's try this now after we have given the context that the model needs which is my name in the previous messages. We will ask the same question so we'll ask what my name is and the model can respond because it has all of the contacts it needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77fc3fd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T12:15:46.104997Z",
     "iopub.status.busy": "2025-12-31T12:15:46.104739Z",
     "iopub.status.idle": "2025-12-31T12:15:47.440728Z",
     "shell.execute_reply": "2025-12-31T12:15:47.440169Z"
    },
    "papermill": {
     "duration": 0.616259,
     "end_time": "2024-03-03T18:06:59.135502",
     "exception": false,
     "start_time": "2024-03-03T18:06:58.519243",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Youssef! How can I assist you further?\n"
     ]
    }
   ],
   "source": [
    "messages =  [  \n",
    "{'role':'system', 'content':'You are friendly chatbot.'},\n",
    "{'role':'user', 'content':'Hi, my name is Youssef'},\n",
    "{'role':'assistant', 'content': \"Hi Youssef! It's nice to meet you. \\\n",
    "Is there anything I can help you with today?\"},\n",
    "{'role':'user', 'content':'Yes, you can remind me, What is my name?'}  ]\n",
    "response = get_completion_from_messages(messages, temperature=1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866f9b03",
   "metadata": {
    "papermill": {
     "duration": 0.010426,
     "end_time": "2024-03-03T18:06:59.156582",
     "exception": false,
     "start_time": "2024-03-03T18:06:59.146156",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"3\"></a>\n",
    "# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b>3. Build a Customized  Chatbot </b></div>\n",
    "\n",
    "\n",
    "Now you're going to build our chatbot this chatbot is going to be called Pizzabot and we're going to automate the collection of user prompts and assistant responses to build this Pizzabot which is going to take orders at a pizza restaurant.\n",
    "First, we're going to define the collect_messages helper function which will collect our user messages so we can avoid typing them in by hand in the same way that we did above. So it is going to collect prompts from a user interface that will build below and then append it to a list called context and then it will call the model with that context every time.\n",
    "\n",
    "The model response is then also added to the contacts so the model message is added to the context the user message is added to the context and so on.  Therefore  it just kind of grows longer and longer this way the model has the information it needs to determine what to do next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "035324b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T12:15:47.443329Z",
     "iopub.status.busy": "2025-12-31T12:15:47.442788Z",
     "iopub.status.idle": "2025-12-31T12:15:47.448677Z",
     "shell.execute_reply": "2025-12-31T12:15:47.447623Z"
    },
    "papermill": {
     "duration": 0.021906,
     "end_time": "2024-03-03T18:06:59.188565",
     "exception": false,
     "start_time": "2024-03-03T18:06:59.166659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "context = [ {'role':'system', 'content':\"\"\"\n",
    "You are OrderBot, an automated service to collect orders for a pizza restaurant. \\\n",
    "You first greet the customer, then collects the order, \\\n",
    "and then asks if it's a pickup or delivery. \\\n",
    "You wait to collect the entire order, then summarize it and check for a final \\\n",
    "time if the customer wants to add anything else. \\\n",
    "If it's a delivery, you ask for an address. \\\n",
    "Finally you collect the payment.\\\n",
    "Make sure to clarify all options, extras and sizes to uniquely \\\n",
    "identify the item from the menu.\\\n",
    "You respond in a short, very conversational friendly style. \\\n",
    "The menu includes \\\n",
    "pepperoni pizza  12.95, 10.00, 7.00 \\\n",
    "cheese pizza   10.95, 9.25, 6.50 \\\n",
    "eggplant pizza   11.95, 9.75, 6.75 \\\n",
    "fries 4.50, 3.50 \\\n",
    "greek salad 7.25 \\\n",
    "Toppings: \\\n",
    "extra cheese 2.00, \\\n",
    "mushrooms 1.50 \\\n",
    "sausage 3.00 \\\n",
    "canadian bacon 3.50 \\\n",
    "AI sauce 1.50 \\\n",
    "peppers 1.00 \\\n",
    "Drinks: \\\n",
    "coke 3.00, 2.00, 1.00 \\\n",
    "sprite 3.00, 2.00, 1.00 \\\n",
    "bottled water 5.00 \\\n",
    "\"\"\"} ]  # accumulate messages\n",
    "panels = [] # collect display\n",
    "\n",
    "def collect_messages(_):\n",
    "    prompt = inp.value_input\n",
    "    inp.value = ''\n",
    "    context.append({'role':'user', 'content':f\"{prompt}\"})\n",
    "    response = get_completion_from_messages(context) \n",
    "    context.append({'role':'assistant', 'content':f\"{response}\"})\n",
    "    panels.append(\n",
    "        pn.Row('User:', pn.pane.Markdown(prompt, width=600)))\n",
    "    panels.append(\n",
    "        pn.Row('Assistant:', pn.pane.Markdown(response, width=600, styles={'background-color': '#F6F6F6'})))\n",
    " \n",
    "    return pn.Column(*panels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7553f34",
   "metadata": {
    "papermill": {
     "duration": 0.00995,
     "end_time": "2024-03-03T18:06:59.208675",
     "exception": false,
     "start_time": "2024-03-03T18:06:59.198725",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now we will set up and run the UI to display the Pizzabot here's the context and it contains the system message that contains the menu note that every time we call the language model we're going to use the same context and the context is building up over time.Â \n",
    "Â The system message to the Pizzabot service is designed to collect orders for a pizza restaurant. We first asked the system to greet the customer and and collect the order and ask if it's a pickup or delivery and you wait to collect the entire order. Then you will summarize it and check for a final time if the customer wants to add anything else if it's a delivery you can ask for an address. Finally, you collect the payment and make sure to clarify all options extras, and sizes to uniquely identify the item from the menu. You respond in a short very conversational friendly style the menu includes and then here we have the menu\n",
    "After executing it we can see the chatbot GUI below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36bf6f54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T12:15:47.453966Z",
     "iopub.status.busy": "2025-12-31T12:15:47.453015Z",
     "iopub.status.idle": "2025-12-31T12:15:47.496268Z",
     "shell.execute_reply": "2025-12-31T12:15:47.495778Z"
    },
    "papermill": {
     "duration": 2.961113,
     "end_time": "2024-03-03T18:07:02.179847",
     "exception": false,
     "start_time": "2024-03-03T18:06:59.218734",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_5264\\312141790.py:41: UserWarning: Using Panel interactively in VSCode notebooks requires the jupyter_bokeh package to be installed. You can install it with:\n",
      "\n",
      "   pip install jupyter_bokeh\n",
      "\n",
      "or:\n",
      "    conda install jupyter_bokeh\n",
      "\n",
      "and try again.\n",
      "  pn.extension()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script type=\"esms-options\">{\"shimMode\": true}</script><style>*[data-root-id],\n",
       "*[data-root-id] > * {\n",
       "  box-sizing: border-box;\n",
       "  font-family: var(--jp-ui-font-family);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
       "}\n",
       "\n",
       "/* Override VSCode background color */\n",
       ".cell-output-ipywidget-background:has(\n",
       "    > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
       "  ),\n",
       ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n  const py_version = '3.6.2'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  const reloading = false;\n  const Bokeh = root.Bokeh;\n\n  // Set a timeout for this load but only if we are not already initializing\n  if (typeof (root._bokeh_timeout) === \"undefined\" || (force || !root._bokeh_is_initializing)) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      // Don't load bokeh if it is still initializing\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    } else if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      // There is nothing to load\n      run_callbacks();\n      return null;\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error(e) {\n      const src_el = e.srcElement\n      console.error(\"failed to load \" + (src_el.href || src_el.src));\n    }\n\n    const skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {}, 'shim': {}});\n      root._bokeh_is_loading = css_urls.length + 0;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    const existing_stylesheets = []\n    const links = document.getElementsByTagName('link')\n    for (let i = 0; i < links.length; i++) {\n      const link = links[i]\n      if (link.href != null) {\n        existing_stylesheets.push(link.href)\n      }\n    }\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const escaped = encodeURI(url)\n      if (existing_stylesheets.indexOf(escaped) !== -1) {\n        on_load()\n        continue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    var existing_scripts = []\n    const scripts = document.getElementsByTagName('script')\n    for (let i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n        existing_scripts.push(script.src)\n      }\n    }\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const escaped = encodeURI(url)\n      if (skip.indexOf(escaped) !== -1 || existing_scripts.indexOf(escaped) !== -1) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (let i = 0; i < js_modules.length; i++) {\n      const url = js_modules[i];\n      const escaped = encodeURI(url)\n      if (skip.indexOf(escaped) !== -1 || existing_scripts.indexOf(escaped) !== -1) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      const url = js_exports[name];\n      const escaped = encodeURI(url)\n      if (skip.indexOf(escaped) >= 0 || root[name] != null) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.holoviz.org/panel/1.7.0/dist/bundled/reactiveesm/es-module-shims@^1.10.0/dist/es-module-shims.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-3.6.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.6.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.6.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.6.2.min.js\", \"https://cdn.holoviz.org/panel/1.7.0/dist/panel.min.js\"];\n  const js_modules = [];\n  const js_exports = {};\n  const css_urls = [];\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (let i = 0; i < inline_js.length; i++) {\n        try {\n          inline_js[i].call(root, root.Bokeh);\n        } catch(e) {\n          if (!reloading) {\n            throw e;\n          }\n        }\n      }\n      // Cache old bokeh versions\n      if (Bokeh != undefined && !reloading) {\n        var NewBokeh = root.Bokeh;\n        if (Bokeh.versions === undefined) {\n          Bokeh.versions = new Map();\n        }\n        if (NewBokeh.version !== Bokeh.version) {\n          Bokeh.versions.set(NewBokeh.version, NewBokeh)\n        }\n        root.Bokeh = Bokeh;\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      // If the timeout and bokeh was not successfully loaded we reset\n      // everything and try loading again\n      root._bokeh_timeout = Date.now() + 5000;\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      root._bokeh_is_loading = 0\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      root._bokeh_is_initializing = true\n      root._bokeh_onload_callbacks = []\n      const bokeh_loaded = root.Bokeh != null && (root.Bokeh.version === py_version || (root.Bokeh.versions !== undefined && root.Bokeh.versions.has(py_version)));\n      if (!reloading && !bokeh_loaded) {\n        if (root.Bokeh) {\n          root.Bokeh = undefined;\n        }\n        console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n        console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n        run_inline_js();\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='0e27abdd-876f-470f-ac8b-9effec00a5dc'>\n",
       "  <div id=\"bfbe6116-3f45-4c42-af7e-12ac6958c1e2\" data-root-id=\"0e27abdd-876f-470f-ac8b-9effec00a5dc\" style=\"display: contents;\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  var docs_json = {\"af314392-4588-4f3d-bea2-907bb54325af\":{\"version\":\"3.6.2\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"panel.models.browser.BrowserInfo\",\"id\":\"0e27abdd-876f-470f-ac8b-9effec00a5dc\"},{\"type\":\"object\",\"name\":\"panel.models.comm_manager.CommManager\",\"id\":\"368492a1-99d9-4076-8282-fca79ccff991\",\"attributes\":{\"plot_id\":\"0e27abdd-876f-470f-ac8b-9effec00a5dc\",\"comm_id\":\"b58ff2550e08469c834728fd719f5257\",\"client_comm_id\":\"761ab931198b47699ffbc77652d42552\"}}],\"defs\":[{\"type\":\"model\",\"name\":\"ReactiveHTML1\"},{\"type\":\"model\",\"name\":\"FlexBox1\",\"properties\":[{\"name\":\"align_content\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"align_items\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"flex_direction\",\"kind\":\"Any\",\"default\":\"row\"},{\"name\":\"flex_wrap\",\"kind\":\"Any\",\"default\":\"wrap\"},{\"name\":\"gap\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"justify_content\",\"kind\":\"Any\",\"default\":\"flex-start\"}]},{\"type\":\"model\",\"name\":\"FloatPanel1\",\"properties\":[{\"name\":\"config\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"contained\",\"kind\":\"Any\",\"default\":true},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"right-top\"},{\"name\":\"offsetx\",\"kind\":\"Any\",\"default\":null},{\"name\":\"offsety\",\"kind\":\"Any\",\"default\":null},{\"name\":\"theme\",\"kind\":\"Any\",\"default\":\"primary\"},{\"name\":\"status\",\"kind\":\"Any\",\"default\":\"normalized\"}]},{\"type\":\"model\",\"name\":\"GridStack1\",\"properties\":[{\"name\":\"ncols\",\"kind\":\"Any\",\"default\":null},{\"name\":\"nrows\",\"kind\":\"Any\",\"default\":null},{\"name\":\"allow_resize\",\"kind\":\"Any\",\"default\":true},{\"name\":\"allow_drag\",\"kind\":\"Any\",\"default\":true},{\"name\":\"state\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"drag1\",\"properties\":[{\"name\":\"slider_width\",\"kind\":\"Any\",\"default\":5},{\"name\":\"slider_color\",\"kind\":\"Any\",\"default\":\"black\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":50}]},{\"type\":\"model\",\"name\":\"click1\",\"properties\":[{\"name\":\"terminal_output\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"debug_name\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"clears\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"FastWrapper1\",\"properties\":[{\"name\":\"object\",\"kind\":\"Any\",\"default\":null},{\"name\":\"style\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"NotificationArea1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"max_notifications\",\"kind\":\"Any\",\"default\":5},{\"name\":\"notifications\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0},{\"name\":\"types\",\"kind\":\"Any\",\"default\":[{\"type\":\"map\",\"entries\":[[\"type\",\"warning\"],[\"background\",\"#ffc107\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-exclamation-triangle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]},{\"type\":\"map\",\"entries\":[[\"type\",\"info\"],[\"background\",\"#007bff\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-info-circle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]}]}]},{\"type\":\"model\",\"name\":\"Notification\",\"properties\":[{\"name\":\"background\",\"kind\":\"Any\",\"default\":null},{\"name\":\"duration\",\"kind\":\"Any\",\"default\":3000},{\"name\":\"icon\",\"kind\":\"Any\",\"default\":null},{\"name\":\"message\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"notification_type\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_rendered\",\"kind\":\"Any\",\"default\":false},{\"name\":\"_destroyed\",\"kind\":\"Any\",\"default\":false}]},{\"type\":\"model\",\"name\":\"TemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"BootstrapTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"TemplateEditor1\",\"properties\":[{\"name\":\"layout\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"MaterialTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"ReactiveESM1\",\"properties\":[{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"JSComponent1\",\"properties\":[{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"ReactComponent1\",\"properties\":[{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"AnyWidgetComponent1\",\"properties\":[{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"request_value1\",\"properties\":[{\"name\":\"fill\",\"kind\":\"Any\",\"default\":\"none\"},{\"name\":\"_synced\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_request_sync\",\"kind\":\"Any\",\"default\":0}]}]}};\n",
       "  var render_items = [{\"docid\":\"af314392-4588-4f3d-bea2-907bb54325af\",\"roots\":{\"0e27abdd-876f-470f-ac8b-9effec00a5dc\":\"bfbe6116-3f45-4c42-af7e-12ac6958c1e2\"},\"root_ids\":[\"0e27abdd-876f-470f-ac8b-9effec00a5dc\"]}];\n",
       "  var docs = Object.values(docs_json)\n",
       "  if (!docs) {\n",
       "    return\n",
       "  }\n",
       "  const py_version = docs[0].version.replace('rc', '-rc.').replace('.dev', '-dev.')\n",
       "  async function embed_document(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    await Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "    for (const render_item of render_items) {\n",
       "      for (const root_id of render_item.root_ids) {\n",
       "\tconst id_el = document.getElementById(root_id)\n",
       "\tif (id_el.children.length && id_el.children[0].hasAttribute('data-root-id')) {\n",
       "\t  const root_el = id_el.children[0]\n",
       "\t  root_el.id = root_el.id + '-rendered'\n",
       "\t  for (const child of root_el.children) {\n",
       "            // Ensure JupyterLab does not capture keyboard shortcuts\n",
       "            // see: https://jupyterlab.readthedocs.io/en/4.1.x/extension/notebook.html#keyboard-interaction-model\n",
       "\t    child.setAttribute('data-lm-suppress-shortcuts', 'true')\n",
       "\t  }\n",
       "\t}\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  function get_bokeh(root) {\n",
       "    if (root.Bokeh === undefined) {\n",
       "      return null\n",
       "    } else if (root.Bokeh.version !== py_version) {\n",
       "      if (root.Bokeh.versions === undefined || !root.Bokeh.versions.has(py_version)) {\n",
       "\treturn null\n",
       "      }\n",
       "      return root.Bokeh.versions.get(py_version);\n",
       "    } else if (root.Bokeh.version === py_version) {\n",
       "      return root.Bokeh\n",
       "    }\n",
       "    return null\n",
       "  }\n",
       "  function is_loaded(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    return (Bokeh != null && Bokeh.Panel !== undefined)\n",
       "  }\n",
       "  if (is_loaded(root)) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (is_loaded(root)) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 200) {\n",
       "          clearInterval(timer);\n",
       "\t  var Bokeh = get_bokeh(root)\n",
       "\t  if (Bokeh == null || Bokeh.Panel == null) {\n",
       "            console.warn(\"Panel: ERROR: Unable to run Panel code because Bokeh or Panel library is missing\");\n",
       "\t  } else {\n",
       "\t    console.warn(\"Panel: WARNING: Attempting to render but not all required libraries could be resolved.\")\n",
       "\t    embed_document(root)\n",
       "\t  }\n",
       "        }\n",
       "      }\n",
       "    }, 25, root)\n",
       "  }\n",
       "})(window);</script>"
      ]
     },
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "0e27abdd-876f-470f-ac8b-9effec00a5dc"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import utils\n",
    "import panel as pn\n",
    "from openai import OpenAI\n",
    "from dotenv import dotenv_values\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ==============================\n",
    "# ðŸ”‘ Load environment variables\n",
    "# ==============================\n",
    "env_values = dotenv_values(\".env\")\n",
    "\n",
    "openai_api_key = env_values.get(\"OPENAI_API_KEY\")\n",
    "openai_api_base = env_values.get(\"OPENAI_API_BASE\")\n",
    "openai_api_name = env_values.get(\"OPENAI_API_NAME\")\n",
    "\n",
    "# Initialize Clients\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base\n",
    ")\n",
    "\n",
    "# Mock moderation for OpenRouter compatibility\n",
    "if openai_api_base and \"openrouter.ai\" in openai_api_base:\n",
    "    class MockResult(dict):\n",
    "        def __getattr__(self, name): return self.get(name)\n",
    "    class MockModeration(dict):\n",
    "        def __init__(self):\n",
    "            res = MockResult(flagged=False, categories={}, category_scores={})\n",
    "            super().__init__(results=[res])\n",
    "        @property\n",
    "        def results(self): return self[\"results\"]\n",
    "    client.moderations.create = lambda *args, **kwargs: MockModeration()\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model_name=openai_api_name, \n",
    "    openai_api_key=openai_api_key, \n",
    "    openai_api_base=openai_api_base,\n",
    "    temperature=0.7\n",
    ")\n",
    "pn.extension()\n",
    "\n",
    "# ==============================\n",
    "# ðŸ”§ Helper Functions\n",
    "# ==============================\n",
    "def get_completion(prompt, model=openai_api_name, temperature=0, max_tokens=500):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def get_completion_from_messages(messages, model=openai_api_name, temperature=0, max_tokens=500):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb693eb",
   "metadata": {
    "papermill": {
     "duration": 0.012882,
     "end_time": "2024-03-03T18:07:02.205872",
     "exception": false,
     "start_time": "2024-03-03T18:07:02.192990",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I am going to say to the chatbot \"Hi, I would like to order a pizza\" and the assistant will respond \"Great choice! Which pizza would you like to order? We have pepperoni pizza for $12.95, cheese pizza for $10.95, and eggplant pizza for $11.95.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beffd7c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T09:18:19.478829Z",
     "iopub.status.busy": "2024-02-28T09:18:19.478500Z",
     "iopub.status.idle": "2024-02-28T09:18:19.486400Z",
     "shell.execute_reply": "2024-02-28T09:18:19.484984Z",
     "shell.execute_reply.started": "2024-02-28T09:18:19.478804Z"
    },
    "papermill": {
     "duration": 0.013473,
     "end_time": "2024-03-03T18:07:02.232399",
     "exception": false,
     "start_time": "2024-03-03T18:07:02.218926",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Next, we will order a medium cheese Pizza, and here is the response:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1024081d",
   "metadata": {
    "papermill": {
     "duration": 0.01261,
     "end_time": "2024-03-03T18:07:02.258602",
     "exception": false,
     "start_time": "2024-03-03T18:07:02.245992",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We can alos order some sides such as water and fries:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a112728e",
   "metadata": {
    "papermill": {
     "duration": 0.012783,
     "end_time": "2024-03-03T18:07:02.284240",
     "exception": false,
     "start_time": "2024-03-03T18:07:02.271457",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now we will finalize the order by saying no I do not want anything else and choose the delivery and payment method:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3ed8e4",
   "metadata": {
    "papermill": {
     "duration": 0.012742,
     "end_time": "2024-03-03T18:07:02.309918",
     "exception": false,
     "start_time": "2024-03-03T18:07:02.297176",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Finally, we can ask the model to create a  Json summary for our order that we can send to the order system based on the conversation. We will append another system message in which we give it instructions to  create a Json summary of the previous food order items and the price for each item. The field should include the pizza ordered, the list of toppings, the list of drinks, the list of sides, and finally the total price.\n",
    "We will use a lower temperature because, for this kind of task, we want the output to be fairly predictable for a conversational agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adad02ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T12:15:47.499971Z",
     "iopub.status.busy": "2025-12-31T12:15:47.499568Z",
     "iopub.status.idle": "2025-12-31T12:15:48.708578Z",
     "shell.execute_reply": "2025-12-31T12:15:48.707947Z"
    },
    "papermill": {
     "duration": 2.635665,
     "end_time": "2024-03-03T18:07:04.958731",
     "exception": false,
     "start_time": "2024-03-03T18:07:02.323066",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Let's get started with your pizza order! ðŸ• \n",
      "\n",
      "What kind of pizza would you like? We have pepperoni, cheese, and eggplant. And what size do you prefer? We have small, medium, and large options!\n"
     ]
    }
   ],
   "source": [
    "messages =  context.copy()\n",
    "messages.append(\n",
    "{'role':'system', 'content':'create a json summary of the previous food order. Itemize the price for each item\\\n",
    " The fields should be 1) pizza, include size 2) list of toppings 3) list of drinks, include size   4) list of sides include size  5)total price '},    \n",
    ")\n",
    " #The fields should be 1) pizza, price 2) list of toppings 3) list of drinks, include size include price  4) list of sides include size include price, 5)total price '},    \n",
    "\n",
    "response = get_completion_from_messages(messages, temperature=0)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b721906",
   "metadata": {
    "papermill": {
     "duration": 0.014241,
     "end_time": "2024-03-03T18:07:04.987077",
     "exception": false,
     "start_time": "2024-03-03T18:07:04.972836",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In summary, we have it you've built your very own order chatbot. Feel free to kind of customize it yourself and play around with the system message to change the behavior of the chatbot and make it act with different personas and different knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c07c52",
   "metadata": {
    "papermill": {
     "duration": 0.015371,
     "end_time": "2024-03-03T18:07:05.015814",
     "exception": false,
     "start_time": "2024-03-03T18:07:05.000443",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30664,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 31.397559,
   "end_time": "2024-03-03T18:07:05.853921",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-03T18:06:34.456362",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
