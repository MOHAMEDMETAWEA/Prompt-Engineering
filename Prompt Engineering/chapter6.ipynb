{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82z0Tuv6Po-A"
   },
   "source": [
    "##  Benefits of Using Output Parsers in LangChain\n",
    "\n",
    "1. **Output Format Control**  \n",
    "   Instead of getting free-form text from the LLM, you can enforce a specific structure such as:\n",
    "   - A list  \n",
    "   - JSON  \n",
    "   - Comma-separated values  \n",
    "   This makes the output predictable and easier to work with programmatically.\n",
    "\n",
    "2. **Easier Post-processing**  \n",
    "   Without a parser, you’d need to manually clean the text (remove numbers, bullets, or formatting).  \n",
    "   The parser automatically converts the model’s response into structured data (like a Python list or dictionary).\n",
    "\n",
    "3. **Consistency**  \n",
    "   LLMs can return results in slightly different formats each time.  \n",
    "   Using an Output Parser ensures consistent and reliable formatting across runs.\n",
    "\n",
    "4. **Reduced Parsing Errors**  \n",
    "   Instead of relying on manual string operations or regex,  \n",
    "   parsers handle the formatting intelligently — minimizing potential parsing mistakes.\n",
    "\n",
    "5. **Seamless Integration**  \n",
    "   The parsed output can be used directly in your code or pipeline  \n",
    "   (for example, as Python lists, JSON objects, or parameters in API calls).\n",
    "\n",
    "6. **Efficiency**  \n",
    "   Saves developer time by removing the need for additional text-cleaning or transformation steps after every LLM call.\n",
    "\n",
    "7. **Scalability & Customization**  \n",
    "   You can build custom parsers to fit your project’s needs (e.g., return data in a specific JSON schema).  \n",
    "   This makes LLM-based applications more flexible and scalable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-13T10:19:37.20496Z",
     "iopub.status.busy": "2025-10-13T10:19:37.204243Z",
     "iopub.status.idle": "2025-10-13T10:19:44.806607Z",
     "shell.execute_reply": "2025-10-13T10:19:44.805825Z",
     "shell.execute_reply.started": "2025-10-13T10:19:37.204929Z"
    },
    "id": "p9ePVZNtPo-C",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -U langchain-community\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T10:19:44.808231Z",
     "iopub.status.busy": "2025-10-13T10:19:44.80793Z",
     "iopub.status.idle": "2025-10-13T10:19:48.159698Z",
     "shell.execute_reply": "2025-10-13T10:19:48.1588Z",
     "shell.execute_reply.started": "2025-10-13T10:19:44.808205Z"
    },
    "id": "bXICOCrZPo-D",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "! pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T10:19:49.804577Z",
     "iopub.status.busy": "2025-10-13T10:19:49.803759Z",
     "iopub.status.idle": "2025-10-13T10:21:17.638014Z",
     "shell.execute_reply": "2025-10-13T10:21:17.637244Z",
     "shell.execute_reply.started": "2025-10-13T10:19:49.804543Z"
    },
    "id": "vULUUp0yPo-E",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q --upgrade transformers accelerate\n",
    "! pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T16:31:49.406484Z",
     "iopub.status.busy": "2025-10-12T16:31:49.405824Z",
     "iopub.status.idle": "2025-10-12T16:31:50.830877Z",
     "shell.execute_reply": "2025-10-12T16:31:50.829915Z",
     "shell.execute_reply.started": "2025-10-12T16:31:49.406448Z"
    },
    "id": "wo2uCgZrPo-E",
    "outputId": "e7a3963e-1bfb-4c7d-9e7f-a6a5c554ca84",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping bitsandbytes as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall bitsandbytes -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T10:21:23.396435Z",
     "iopub.status.busy": "2025-10-13T10:21:23.395815Z",
     "iopub.status.idle": "2025-10-13T10:21:29.199366Z",
     "shell.execute_reply": "2025-10-13T10:21:29.198569Z",
     "shell.execute_reply.started": "2025-10-13T10:21:23.396404Z"
    },
    "id": "3PSH9AikPo-E",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "! pip install bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "5332419b121c4dfdb93dccbccf8efee6",
      "edf1bd54d6ac4611842203e60920fc37",
      "fac65c863fbb4ae4aeaedd4ac956fbbf",
      "5cb823182d21449bb567955d056a7159",
      "66e7ebbdf74c4601a80e13c64b4a2403",
      "f134838fe97d4d29b9d8d905f81a5384",
      "64a905933d08461e9b34de50bac2c6ec",
      "fa4eb3c9be26459d83f4e4130d219d7d",
      "b4bfde9ad3bd420ba405979e5c0753b1",
      "95feecf72bc64082b7a8feef5cc969f1",
      "a66f9cae4a7c454a8e1bd2c899f57cd0",
      "607d3787c58f4899bb93ecd41b57132e",
      "4a884dba418c44699967264eb4cb861f",
      "ab0e8b88b41a4eeeb7a20dbfc90dfd65"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-10-13T10:21:34.67183Z",
     "iopub.status.busy": "2025-10-13T10:21:34.671538Z",
     "iopub.status.idle": "2025-10-13T10:24:29.855447Z",
     "shell.execute_reply": "2025-10-13T10:24:29.85477Z",
     "shell.execute_reply.started": "2025-10-13T10:21:34.671807Z"
    },
    "id": "CsC9zbhJPo-F",
    "outputId": "57492806-5b70-4c65-eb13-460e7633a09f",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-13 10:21:45.123505: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1760350905.294723      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1760350905.342068      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5332419b121c4dfdb93dccbccf8efee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf1bd54d6ac4611842203e60920fc37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fac65c863fbb4ae4aeaedd4ac956fbbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/643 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb823182d21449bb567955d056a7159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/557 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e7ebbdf74c4601a80e13c64b4a2403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f134838fe97d4d29b9d8d905f81a5384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64a905933d08461e9b34de50bac2c6ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa4eb3c9be26459d83f4e4130d219d7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4bfde9ad3bd420ba405979e5c0753b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95feecf72bc64082b7a8feef5cc969f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/2.68G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a66f9cae4a7c454a8e1bd2c899f57cd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/3.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "607d3787c58f4899bb93ecd41b57132e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/3.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a884dba418c44699967264eb4cb861f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab0e8b88b41a4eeeb7a20dbfc90dfd65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/136 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "model_id = \"NousResearch/Hermes-2-Pro-Mistral-7B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_fast=False,\n",
    "    trust_remote_code=True,\n",
    "    revision=\"main\"\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    revision=\"main\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T10:24:33.507399Z",
     "iopub.status.busy": "2025-10-13T10:24:33.50638Z",
     "iopub.status.idle": "2025-10-13T10:24:34.600154Z",
     "shell.execute_reply": "2025-10-13T10:24:34.599408Z",
     "shell.execute_reply.started": "2025-10-13T10:24:33.507372Z"
    },
    "id": "z39GtWKyPo-F",
    "outputId": "bf06a3cc-a4df-4b0f-b494-6d8e43c191b1",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/tmp/ipykernel_37/1013225100.py:12: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llms =HuggingFacePipeline(pipeline=pipe)\n"
     ]
    }
   ],
   "source": [
    "from transformers  import AutoTokenizer , AutoModelForCausalLM, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "pipe=pipeline(\"text-generation\",\n",
    "              model=base_model,\n",
    "\n",
    "tokenizer=tokenizer,\n",
    "temperature=0.5\n",
    "\n",
    ")\n",
    "\n",
    "llms =HuggingFacePipeline(pipeline=pipe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6RIg31SPo-F"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T10:25:03.15957Z",
     "iopub.status.busy": "2025-10-13T10:25:03.159012Z",
     "iopub.status.idle": "2025-10-13T10:25:03.16427Z",
     "shell.execute_reply": "2025-10-13T10:25:03.163528Z",
     "shell.execute_reply.started": "2025-10-13T10:25:03.159542Z"
    },
    "id": "IM_zaUQKPo-G",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "format_instructions=output_parser.get_format_instructions\n",
    "\n",
    "prompt1 = PromptTemplate(\n",
    "    template = \"List three popular {plate_type} plates.\\n{format_instructions}\",\n",
    "    input_variables=[\"plate_type\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "prompt2 = PromptTemplate(\n",
    "    template = \"List three popular {plate_type} plates\",\n",
    "    input_variables=[\"plate_type\"]\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T10:25:06.340011Z",
     "iopub.status.busy": "2025-10-13T10:25:06.339733Z",
     "iopub.status.idle": "2025-10-13T10:25:06.344985Z",
     "shell.execute_reply": "2025-10-13T10:25:06.344104Z",
     "shell.execute_reply.started": "2025-10-13T10:25:06.339989Z"
    },
    "id": "XxyUE0zkPo-G",
    "outputId": "7314023b-1fb8-4e0e-a186-272e41f23ae2",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using outputpaser:     input_variables=['plate_type'] input_types={} partial_variables={'format_instructions': <bound method CommaSeparatedListOutputParser.get_format_instructions of CommaSeparatedListOutputParser()>} template='List three popular {plate_type} plates.\\n{format_instructions}'\n",
      "\n",
      "============================================\n",
      "without outputpaser:        input_variables=['plate_type'] input_types={} partial_variables={} template='List three popular {plate_type} plates'\n"
     ]
    }
   ],
   "source": [
    "print (f\"using outputpaser:     {prompt1}\")\n",
    "print ()\n",
    "print (\"============================================\")\n",
    "\n",
    "print (f\"without outputpaser:        {prompt2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T10:25:34.421359Z",
     "iopub.status.busy": "2025-10-13T10:25:34.421029Z",
     "iopub.status.idle": "2025-10-13T10:25:41.003363Z",
     "shell.execute_reply": "2025-10-13T10:25:41.002645Z",
     "shell.execute_reply.started": "2025-10-13T10:25:34.421333Z"
    },
    "id": "PWaW2qoAPo-G",
    "outputId": "4c2c3543-4992-4fd5-e975-d76874ea5484",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37/156191283.py:3: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llms( demo_message )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['List three popular arabian plates.', 'Your response should be a list of comma separated values', 'eg: `foo', 'bar', 'baz` or `foo', 'bar', 'baz`', '1. Mezze platter', '2. Mixed grill platter', '3. Shawarma platter', 'The three popular arabian plates are the Mezze platter', 'which typically includes a variety of dips', 'spreads', 'and salads; the Mixed grill platter', 'which features an assortment of grilled meats and vegetables; and the Shawarma platter', 'a stack of thinly sliced marinated meats served with pita bread and various toppings.']\n"
     ]
    }
   ],
   "source": [
    "demo_message = prompt1.format(plate_type='arabian')\n",
    "\n",
    "response = llms( demo_message )\n",
    "\n",
    "output = output_parser.parse(response)\n",
    "\n",
    "print(output)\n",
    "\n",
    "# it give me outpus as list , answers is bad , model is very week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYL-x7GQPo-G"
   },
   "source": [
    "# Data Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T10:26:35.621491Z",
     "iopub.status.busy": "2025-10-13T10:26:35.620937Z",
     "iopub.status.idle": "2025-10-13T10:26:48.159004Z",
     "shell.execute_reply": "2025-10-13T10:26:48.158178Z",
     "shell.execute_reply.started": "2025-10-13T10:26:35.621464Z"
    },
    "id": "-vcwwEaHPo-G",
    "outputId": "26a84bc6-bca4-4ee3-f0e0-30bfcd21e999",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when was thomas edison born?\n",
      "Replay with a datetime format DAY/MONTH/YEAR like `23/05/1988`.\n",
      "Thomas Edison was born on February 11, 1847.\n",
      "Replay with a datetime format: 11/02/1847.\n",
      "Thomas Edison was born on February 11, 1847.\n",
      "Replay with the exact datetime format: 1847-02-11.\n",
      "Thomas Edison was born on February 11, 1847.\n",
      "Replay with a different format: Thomas Edison was born on 11/02/1847.\n",
      "Thomas Edison was born on February 11, 1847.\n",
      "Replay with the datetime in words: Thomas Edison was born on the 11th of February, 1847.\n",
      "Thomas Edison was born on February 11, 1847.\n",
      "Replay with a datetime in a different format: Thomas Edison was born on 1847-02-11.\n",
      "Thomas Edison was born on February 11, 1847.\n",
      "Replay with a datetime format that\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "format_instructions = \"Replay with a datetime format DAY/MONTH/YEAR like `23/05/1988`.\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"when was {person} born?\\n{format_instructions}\",\n",
    "    input_variables=['person'],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")\n",
    "\n",
    "message = prompt.format(person='thomas edison')\n",
    "\n",
    "print( llms( message ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BizyLEBMPo-H"
   },
   "source": [
    "# pydantic\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##  Key Points\n",
    "\n",
    "### 1. Purpose of Pydantic\n",
    "- Helps define structured data models with fixed field types.  \n",
    "- Makes output **consistent**, **validated**, and **easier to use** in code.  \n",
    "- Replaces the idea of just returning raw strings with **well-defined objects**.\n",
    "\n",
    "## ⚠️ Tips & Warnings\n",
    "\n",
    "### ✅ Advantages\n",
    "- Structured and clean data.  \n",
    "- Easy to integrate into real applications or APIs.  \n",
    "\n",
    "### ⚠️ Warnings\n",
    "- Pydantic parsers increase **token usage** because they include examples and schema definitions in the prompt.  \n",
    "- Use them **only when necessary** to avoid high costs.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T11:03:48.375623Z",
     "iopub.status.busy": "2025-10-13T11:03:48.375056Z",
     "iopub.status.idle": "2025-10-13T11:03:48.380827Z",
     "shell.execute_reply": "2025-10-13T11:03:48.380051Z",
     "shell.execute_reply.started": "2025-10-13T11:03:48.3756Z"
    },
    "id": "fg6JFbQFPo-H",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# {\n",
    "#     \"plate_name\": \"Shawerma\",\n",
    "#     \"ingredients\": [\n",
    "#         \"ingredient 1\", \"ingredient 2\"\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "class Plate(BaseModel):\n",
    "    plate_name: str = Field(description=\"name of the plat\")\n",
    "    ingredients: List[str] = Field(description=\"list of names of the ingredients\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Plate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T11:00:08.023794Z",
     "iopub.status.busy": "2025-10-13T11:00:08.023014Z",
     "iopub.status.idle": "2025-10-13T11:00:08.027886Z",
     "shell.execute_reply": "2025-10-13T11:00:08.027058Z",
     "shell.execute_reply.started": "2025-10-13T11:00:08.023758Z"
    },
    "id": "-rGMqFmAPo-H",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=(\n",
    "        \"You are a helpful chef assistant.\\n\"\n",
    "        \"Given the plate name, list its ingredients.\\n\\n\"\n",
    "        \"Plate name: {plate_name}\\n\\n\"\n",
    "        \"{format_instructions}\"\n",
    "    ),\n",
    "    input_variables=[\"plate_name\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T11:00:11.69979Z",
     "iopub.status.busy": "2025-10-13T11:00:11.699539Z",
     "iopub.status.idle": "2025-10-13T11:00:18.190674Z",
     "shell.execute_reply": "2025-10-13T11:00:18.189697Z",
     "shell.execute_reply.started": "2025-10-13T11:00:11.699773Z"
    },
    "id": "HpLWLtXiPo-H",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plate_name = 'Shawerma'\n",
    "\n",
    "message = prompt.format(plate_name=plate_name)\n",
    "\n",
    "response = llms( message )\n",
    "\n",
    "output = parser.parse(response)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "https://storage.googleapis.com/kaggle-colab-exported-notebooks/mohamedehab0122/chapter6.0d692f21-c667-414e-adad-0c5d39f9a7b1.ipynb?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com/20251013/auto/storage/goog4_request&X-Goog-Date=20251013T110446Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=4c229f0da93b31577d06d8060b18e85e86f1f6ee84f9fda615c5bb3f7144a48d803e961d427806f2083172ffee53d3fffeb21a8720dd393a83ee580a248ce246bb4bdf6fea3102b5a35276c21f06a681f65bb24e814886bea0bd316ec4a2e2447006ae8d621ab1da201330da3c58adab1ec97b562ad416e34b17b8c3811ae00d01856b1246a2d0cf653f893e1802c8d9bb3b25cf72a7de3de5dbfbafb479c5e44ddac21cc889ca06528b67987e8edf83761dac4a36554695e266ee1e03007072d02c50ab92a7d0af06a563a3e4204c669ca67e0c6d23144c1663ffb401ff944f1bb23f14693594cafd12d4a1edec9b8b9c4bb37219e199f00a2587305000f7c3",
     "timestamp": 1760353542379
    }
   ]
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
