{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bph0JuLUJBGt"
   },
   "source": [
    "# üß† LangChain Memory Types\n",
    "\n",
    "## 1. Buffer Memory\n",
    "**Definition:**  \n",
    "Stores the entire conversation history in memory and passes all previous messages to the model each time a new response is generated.\n",
    "\n",
    "**Key Points:**\n",
    "- Keeps **all messages** (inputs and outputs).\n",
    "- Can consume **a lot of tokens** if the conversation is long.\n",
    "- Good for **short, context-rich** conversations.\n",
    "\n",
    "**Example Use Case:**  \n",
    "Chatbots that must remember everything said during the current session.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Buffer Window Memory\n",
    "**Definition:**  \n",
    "Stores **only the last N interactions (or turns)** instead of the entire conversation.\n",
    "\n",
    "**Key Points:**\n",
    "- Keeps a **sliding window** of recent messages.\n",
    "- Reduces token usage compared to Buffer Memory.\n",
    "- Good for **long conversations** where only recent context matters.\n",
    "\n",
    "**Example Use Case:**  \n",
    "Support bots or assistants where only the last few user requests are relevant.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Summary Memory\n",
    "**Definition:**  \n",
    "Instead of storing all messages, it creates and maintains a **summary** of the conversation so far.\n",
    "\n",
    "**Key Points:**\n",
    "- Uses a **language model to summarize** previous messages.\n",
    "- Keeps memory **compact** and **token-efficient**.\n",
    "- May **lose details**, since summaries can generalize.\n",
    "\n",
    "**Example Use Case:**  \n",
    "When maintaining long-term context with limited token budget.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Entity Memory\n",
    "**Definition:**  \n",
    "Tracks and remembers **specific entities** (people, places, things) mentioned in the conversation.\n",
    "\n",
    "**Key Points:**\n",
    "- Extracts entities and stores **facts or attributes** about them.\n",
    "- Helps models remember **who is who** and **what was said** about each entity.\n",
    "- Useful for **personalized or knowledge-based** chats.\n",
    "\n",
    "**Example Use Case:**  \n",
    "Virtual assistants that need to recall user preferences, names, or prior facts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "id": "OZ8PpAQcJBGw",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -U langchain-community\n",
    "! pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HAGWWSOMJBGy",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q --upgrade transformers accelerate\n",
    "! pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qT5f_cfIJBGy",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "! pip install bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PWCL6ypCJBGz",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ImiDBB2QJBGz",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -U flash-attn --no-build-isolation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Load environment variables\n",
    "# ==============================\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "# Load variables from app.env\n",
    "env_vars = dotenv_values(\"app.env\")\n",
    "print(\"Loaded ENV variables:\", env_vars)\n",
    "\n",
    "# Extract required variables\n",
    "openai_api_key = env_vars[\"OPENAI_API_KEY\"]\n",
    "openai_api_base = env_vars.get(\"OPENAI_API_BASE\")\n",
    "openai_api_name = env_vars[\"OPENAI_API_Name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Create LangChain Chat Model\n",
    "# ==============================\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model=openai_api_name,\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    "    temperature=0.7,\n",
    "    max_tokens=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversation Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new answer has relation to the previous cells\n",
    "# ==============================\n",
    "\n",
    "message_1 = \"Can you list 3 places to visit in France?\"\n",
    "response_1 = chat.invoke(message_1)\n",
    "print(\"AI Response:\", response_1.content)\n",
    "# =============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_2 = \"Which one is the most popular?\"\n",
    "response_2 = chat.invoke(message_2)\n",
    "print(\"AI Response:\", response_2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langchain-core langchain-openai langchain-community python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install latest packages if needed\n",
    "%pip install -U langchain-core langchain-openai langchain-community python-dotenv\n",
    "\n",
    "# Use existing `history` (InMemoryChatMessageHistory) and `conversation` (RunnableWithMessageHistory)\n",
    "# defined in other cells. Use `session_id` from other cell (e.g. \"session_1\").\n",
    "\n",
    "def send_message(message, session_id=\"session_1\"):\n",
    "    # Invoke the RunnableWithMessageHistory which will use the shared history\n",
    "    response = conversation.invoke(\n",
    "        {\"input\": [message]},\n",
    "        config={\"configurable\": {\"session_id\": session_id}}\n",
    "    )\n",
    "    output = response[\"output\"]\n",
    "\n",
    "    # Ensure history has the entries (RunnableWithMessageHistory usually handles this,\n",
    "    # but add explicitly if needed)\n",
    "    try:\n",
    "        history.add_user_message(message)\n",
    "        history.add_ai_message(output)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return output\n",
    "\n",
    "# Messages\n",
    "message_1 = \"Can you list 3 places in Paris to visit?\"\n",
    "message_2 = \"Which place has a lake view?\"\n",
    "\n",
    "# Get responses\n",
    "output_1 = send_message(message_1, session_id)\n",
    "print(\"Response 1:\", output_1)\n",
    "\n",
    "output_2 = send_message(message_2, session_id)\n",
    "print(\"Response 2:\", output_2)\n",
    "\n",
    "# Print conversation history\n",
    "print(\"\\nConversation History:\")\n",
    "for msg in history.messages:\n",
    "    print(f\"{msg.type}: {msg.content}\")\n",
    "    print('---------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install latest packages if needed\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Initialize memory\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "# Function to send message and store in memory\n",
    "def send_message(message):\n",
    "    # Combine previous messages from memory\n",
    "    previous_messages = \"\\n\".join(\n",
    "        [f\"{m.type}: {m.content}\" for m in memory.chat_memory.messages]\n",
    "    )\n",
    "    prompt = f\"{previous_messages}\\nHuman: {message}\\nAI:\"\n",
    "\n",
    "    # Get model response\n",
    "    response = llm.call_as_llm(prompt)\n",
    "    \n",
    "    # Add message and response to memory\n",
    "    memory.chat_memory.add_user_message(message)\n",
    "    memory.chat_memory.add_ai_message(response)\n",
    "\n",
    "    return response\n",
    "\n",
    "# Messages\n",
    "message_1 = \"Can you list 3 places in Paris to visit?\"\n",
    "message_2 = \"Which place has a lake view?\"\n",
    "\n",
    "# Get responses\n",
    "output_1 = send_message(message_1)\n",
    "print(\"Response 1:\", output_1)\n",
    "\n",
    "output_2 = send_message(message_2)\n",
    "print(\"Response 2:\", output_2)\n",
    "\n",
    "# Print conversation history\n",
    "print(\"\\nConversation History:\")\n",
    "for msg in memory.chat_memory.messages:\n",
    "    print(f\"{msg.type}: {msg.content}\")\n",
    "    print('---------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Conversation Memory\n",
    "# ------------------------------\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "def get_session_history(session_id):\n",
    "    return history\n",
    "\n",
    "conversation = RunnableWithMessageHistory(\n",
    "    runnable=chat,               \n",
    "    get_session_history=get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "from langchain_openai import ChatOpenAI  # ÿ™ÿ£ŸÉÿØ ŸÖŸÜ ÿ™ÿ´ÿ®Ÿäÿ™ langchain_openai\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model=openai_api_name,\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    "    temperature=0.7,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# ÿ•ÿπÿØÿßÿØ ÿßŸÑÿ∞ÿßŸÉÿ±ÿ©\n",
    "# ------------------------------\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "conversation = RunnableWithMessageHistory(\n",
    "    runnable=chat,                 # ÿßŸÑŸÄ LLM\n",
    "    get_session_history=lambda session_id: history,  # ŸÖÿ®ÿßÿ¥ÿ±ÿ© ŸÑÿ™ÿßÿ±ŸäÿÆ\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# ÿßŸÑŸÖÿ≠ÿßÿØÿ´ÿ©\n",
    "# ------------------------------\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "session_id = \"session_1\"\n",
    "\n",
    "message_1 = \"Can you list 3 places to visit in France?\"\n",
    "message_2 = \"Which one is the most popular?\"\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    # ÿ™ŸÖÿ±Ÿäÿ± dict Ÿäÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ÿßŸÑŸÖŸÅÿ™ÿßÿ≠ 'input'\n",
    "    response_1 = conversation.invoke(\n",
    "        {\"input\": [message_1]},\n",
    "        config={\"configurable\": {\"session_id\": session_id}}\n",
    "    )\n",
    "    print(\"AI Response 1:\", response_1[\"output\"])\n",
    "\n",
    "    response_2 = conversation.invoke(\n",
    "        {\"input\": [message_2]},\n",
    "        config={\"configurable\": {\"session_id\": session_id}}\n",
    "    )\n",
    "    print(\"AI Response 2:\", response_2[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversation Buffer memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "e5a4589e8d934c6bb82cf5892d1b89f0",
      "13c3e8cec29842ab99821ada274bbf33",
      "2c9286b430214bf38d340fa68060a4e3",
      "074d70ed540f4fc19247906e91be7b99",
      "b9de8b068c0f4a5c92d0f637a36bc435",
      "23375cfb99714827953c38a82be2f82e",
      "6cb2b9ca00a04c8084576c9e83a87bd9"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-10-15T18:13:20.857896Z",
     "iopub.status.busy": "2025-10-15T18:13:20.857321Z",
     "iopub.status.idle": "2025-10-15T18:15:02.304372Z",
     "shell.execute_reply": "2025-10-15T18:15:02.303562Z",
     "shell.execute_reply.started": "2025-10-15T18:13:20.857874Z"
    },
    "id": "I1IDauD0JBG0",
    "outputId": "374284ff-765c-4636-d265-eb29ac6deefc",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5a4589e8d934c6bb82cf5892d1b89f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/729 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c3e8cec29842ab99821ada274bbf33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/534k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9286b430214bf38d340fa68060a4e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "074d70ed540f4fc19247906e91be7b99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/775 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9de8b068c0f4a5c92d0f637a36bc435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/6.85G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23375cfb99714827953c38a82be2f82e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/6.85G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb2b9ca00a04c8084576c9e83a87bd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('/kaggle/working/h2ogpt_model/tokenizer_config.json',\n",
       " '/kaggle/working/h2ogpt_model/special_tokens_map.json',\n",
       " '/kaggle/working/h2ogpt_model/tokenizer.model',\n",
       " '/kaggle/working/h2ogpt_model/added_tokens.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_id = \"h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-3b\"\n",
    "save_path = \"/kaggle/working/h2ogpt_model\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Move model to CPU for safe saving\n",
    "model.to(\"cpu\")\n",
    "\n",
    "# Save with safetensors + shards\n",
    "model.save_pretrained(save_path, safe_serialization=True, max_shard_size=\"2GB\")\n",
    "tokenizer.save_pretrained(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IFu6Y6utJBG1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers  import AutoTokenizer , AutoModelForCausalLM, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "pipe=pipeline(\"text-generation\",\n",
    "              model=base_model,\n",
    "\n",
    "tokenizer=tokenizer,\n",
    "temperature=0.5\n",
    "\n",
    ")\n",
    "\n",
    "llms =HuggingFacePipeline(pipeline=pipe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iG3eRootJBG1"
   },
   "source": [
    "# Buffer Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8G8yB7WJBG2"
   },
   "source": [
    "## without  Buffer Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pyC8RFknJBG2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "message_1 = \"Can you list 3 places in Paris to visit?\"\n",
    "\n",
    "message_2 = \"Which place has a lake view?\"\n",
    "\n",
    "print (llms(message_1))\n",
    "print ()\n",
    "print (\"===========================================\")\n",
    "print ()\n",
    "print (llms(message_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0GzW4OKJBG2"
   },
   "source": [
    "## using  Buffer Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r7RTnsV5JBG2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llms,\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "message_1 = \"Can you list 3 places in Paris to visit?\"\n",
    "\n",
    "message_2 = \"Which place has a lake view?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EuvQS5c3JBG2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "output_1 = conversation.predict(input=message_1)\n",
    "print(output_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "agDVSQyVJBG2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "output_2 = conversation.predict(input=message_2)\n",
    "print(output_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xY1tlOnbJBG3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for msg in conversation.memory.chat_memory.messages:\n",
    "    print(msg)\n",
    "    print('---------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5G1tHtBGJBG3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for msg in conversation.memory.chat_memory.messages:\n",
    "    print(msg)\n",
    "    print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3lobCTUJJBG3"
   },
   "source": [
    "# Buffer Window Memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qD9R0fE4JBG3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llms,\n",
    "    memory=ConversationBufferWindowMemory( k=1 ),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1JlL0BdtJBG3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "message_1 = \"Can you list 3 places in Paris to visit?\"\n",
    "\n",
    "message_2 = \"Which place has a lake view?\"\n",
    "\n",
    "message_3 = \"Can I visit in winter?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xYkhZpiQJBG3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print( conversation.predict(input=message_1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qb1KYo4bJBG3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print( conversation.predict(input=message_2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Ope6UYHJBG3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print( conversation.predict(input=message_3) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQ3rQgCnJBG3"
   },
   "source": [
    "# Summary Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wVcRD_MbJBG4",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "conversation = ConversationChain(\n",
    "    llm=llms,\n",
    "    memory=ConversationSummaryBufferMemory(llm=llms), # # can used another model for summary\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "message_1 = \"Can you list 3 places in Paris to visit?\"\n",
    "\n",
    "message_2 = \"Which place has a lake view?\"\n",
    "\n",
    "message_3 = \"Can I visit in winter?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "po7OmtjqJBG4",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print( conversation.predict(input=message_1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bPLT_eGBJBG4",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print( conversation.predict(input=message_2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2XW1h1nCJBG4",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print( conversation.predict(input=message_3) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g19a1eO_JBG4"
   },
   "source": [
    "###  Notes: Why `ConversationSummaryMemory` Uses Many Tokens\n",
    "\n",
    "- `ConversationSummaryMemory` summarizes previous messages using the LLM after each turn.  \n",
    "- This causes **two LLM calls per message** ‚Üí one for generating a reply, and another for updating the summary.  \n",
    "- Summarization consumes tokens because the model must \"read\" previous messages.  \n",
    "- The generated summary is also added to each new prompt, increasing token count.  \n",
    "- LangChain adds extra prompt text (e.g., ‚ÄúHere‚Äôs the summary so far‚Äù), which also uses tokens.  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6NKzyTkJBG4"
   },
   "source": [
    "#  Entity Mmory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CPuaSn75JBG4",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationEntityMemory\n",
    "from langchain.memory.prompt import ENTITY_MEMORY_CONVERSATION_TEMPLATE\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llms,\n",
    "    memory=ConversationEntityMemory(llm=llms),\n",
    "    prompt=ENTITY_MEMORY_CONVERSATION_TEMPLATE,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RY9TSgdqJBG4",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(conversation.predict(input=\"Can you list 3 places in Paris to visit?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Howw3zrSJBG4",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(conversation.memory.entity_store.store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8_St1Q-JBG5"
   },
   "source": [
    "# Save to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gyA3kcOHJBG5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.schema import messages_from_dict, messages_to_dict\n",
    "import json\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llms,\n",
    "    memory=ConversationSummaryMemory(llm=llms),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KJ8osI22JBG5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "message_1 = \"Can you list 3 places in Paris to visit?\"\n",
    "message_2 = \"Which place has a lake view?\"\n",
    "\n",
    "output_1 = conversation.predict(input=message_1)\n",
    "output_2 = conversation.predict(input=message_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QXgz1hHOJBG5",
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9i6t2FKFJBG5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain.schema import message_to_dict, messages_from_dict\n",
    "import json\n",
    "\n",
    "dicts = [message_to_dict(m) for m in conversation.memory.chat_memory.messages]\n",
    "\n",
    "with open(\"/content/conversation-memory.json\", \"w\", encoding=\"utf-8\") as dest:\n",
    "    json.dump(dicts, dest, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ENia0UzGJBG5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "with open(\"/content/conversation-memory.json\") as src:\n",
    "    saved_history = json.loads(src.read())\n",
    "history =ChatMessageHistory()\n",
    "history.messages=messages_from_dict(saved_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z1trVfWnJBHD",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "memory =ConversationSummaryMemory()\n",
    "memory = memory.from_messages(chat_memory=history, llm=llms)\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "memory = memory.from_messages(chat_memory=history, llm=llm)\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "94gJvuv6JBHE",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "message_3 = \"Can I visit in winter?\"\n",
    "\n",
    "print( conversation.predict(input=message_3) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oYYzXSLHJBHE",
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bONp0xl6JBHE"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "https://storage.googleapis.com/kaggle-colab-exported-notebooks/mohamedehab0122/chapter7.49661c8d-d9d8-48da-b558-dec1db5bb399.ipynb?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com/20251015/auto/storage/goog4_request&X-Goog-Date=20251015T183116Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=0c597bcdaf0eecff78f14f615f6b0dbd65f518260361694cefed2c42bec361ef619bcd351ab3d26a7579891cac06bdde5a4ce04c2dd7b0fe28ea4d8e56c0b7ed2de06e59508712d9293ae61997493ee44919b8885116cd4fbbfd832f1893b87a7560bb5b1b5d356872a3b9d7f3544526156a9bf9f0af5826a438b3c5a738bb01f3b30f78f49cd031dc45f35b5feff28855832365858e9ee27741839306608a7b26b96303290567c38adf7a7df7286f95c217263fc858c12eaa35e4e05770237ec970d704195c58dca2150d85ab1fc00f56ef86e8b299ac0d4613bfcda8dad36ef7c6b4b48d2e282d995858ca61ab87c2cc70b45e7cfe8fce965fa3245aafc46a",
     "timestamp": 1760553091292
    }
   ]
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
